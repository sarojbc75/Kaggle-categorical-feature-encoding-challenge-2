{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SC=StandardScaler()\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBRFClassifier,XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# for dealing with imbalanced data\n",
    "from imblearn.under_sampling import NearMiss,RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler,SMOTENC\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Remarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bin_0</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bin_1</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bin_2</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bin_3</td>\n",
       "      <td>Fail to Reject Null Hypothesis: Drop the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bin_4</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nom_0</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nom_1</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nom_2</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nom_3</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nom_4</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nom_5</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nom_6</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nom_7</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nom_8</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nom_9</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ord_0</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ord_1</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ord_2</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ord_3</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ord_4</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ord_5</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>day</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>month</td>\n",
       "      <td>Reject Null Hypothesis: Retain the Feature:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature                                            Remarks\n",
       "0    bin_0        Reject Null Hypothesis: Retain the Feature:\n",
       "1    bin_1        Reject Null Hypothesis: Retain the Feature:\n",
       "2    bin_2        Reject Null Hypothesis: Retain the Feature:\n",
       "3    bin_3  Fail to Reject Null Hypothesis: Drop the Feature:\n",
       "4    bin_4        Reject Null Hypothesis: Retain the Feature:\n",
       "5    nom_0        Reject Null Hypothesis: Retain the Feature:\n",
       "6    nom_1        Reject Null Hypothesis: Retain the Feature:\n",
       "7    nom_2        Reject Null Hypothesis: Retain the Feature:\n",
       "8    nom_3        Reject Null Hypothesis: Retain the Feature:\n",
       "9    nom_4        Reject Null Hypothesis: Retain the Feature:\n",
       "10   nom_5        Reject Null Hypothesis: Retain the Feature:\n",
       "11   nom_6        Reject Null Hypothesis: Retain the Feature:\n",
       "12   nom_7        Reject Null Hypothesis: Retain the Feature:\n",
       "13   nom_8        Reject Null Hypothesis: Retain the Feature:\n",
       "14   nom_9        Reject Null Hypothesis: Retain the Feature:\n",
       "15   ord_0        Reject Null Hypothesis: Retain the Feature:\n",
       "16   ord_1        Reject Null Hypothesis: Retain the Feature:\n",
       "17   ord_2        Reject Null Hypothesis: Retain the Feature:\n",
       "18   ord_3        Reject Null Hypothesis: Retain the Feature:\n",
       "19   ord_4        Reject Null Hypothesis: Retain the Feature:\n",
       "20   ord_5        Reject Null Hypothesis: Retain the Feature:\n",
       "21     day        Reject Null Hypothesis: Retain the Feature:\n",
       "22   month        Reject Null Hypothesis: Retain the Feature:"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Lets test Chisquare test of Independence to find out which all features are of importnce.\n",
    "# Assume Significance level =0.05\n",
    "# Ho: There is no significant dependency between Categorical feature(X1 Or X2...or Xn) and Label:target\n",
    "# H1: There is no  dependency between Categorical feature(X1 Or X2...or Xn) and Label:target\n",
    "# If the Actual p-value computed from Chisquare test is < 0.05 then we reject the NULL hypothesis\n",
    "        #(Means there is dependency betwen X and y and we can retain the feature)\n",
    "# If the Actual p-value computed from Chisquare test is >= 0.05 then we fail to reject the NULL hypothesis\n",
    "         #(Means there is no dependency betwen X and y and we can drop the feature)\n",
    "# The basic requirement of Chisquare test is: All variables should be Categorical.\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "train_chi=pd.read_csv(r'C:\\Saroj_Official\\AI\\Kaggle\\Categorical-Featuer-Encoding-Challange-2\\train.csv').copy()\n",
    "train_chi=train_chi.astype('object').copy()\n",
    "train_chi.dropna(inplace=True)\n",
    "# Select all catagorical columns except target\n",
    "categorical_columns=list(train_chi.select_dtypes(include='object').drop(columns=['id','target']).columns)\n",
    "\n",
    "\n",
    "\n",
    "chi2_check = []\n",
    "for i in categorical_columns:\n",
    "    ch2 , p_value , df, exp_freq=chi2_contingency(pd.crosstab(train_chi[i],train_chi['target']))\n",
    "    if p_value < 0.05:\n",
    "        chi2_check.append('Reject Null Hypothesis: Retain the Feature:')\n",
    "    else:\n",
    "        chi2_check.append('Fail to Reject Null Hypothesis: Drop the Feature:')\n",
    "        \n",
    "result = pd.DataFrame(chi2_check,columns=['Remarks'])\n",
    "result.insert(0,'Feature',categorical_columns)\n",
    "del train_chi\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r'C:\\Saroj_Official\\AI\\Kaggle\\Categorical-Featuer-Encoding-Challange-2\\train.csv')\n",
    "test=pd.read_csv(r'C:\\Saroj_Official\\AI\\Kaggle\\Categorical-Featuer-Encoding-Challange-2\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600000 entries, 0 to 599999\n",
      "Data columns (total 25 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   id      600000 non-null  int64  \n",
      " 1   bin_0   582106 non-null  float64\n",
      " 2   bin_1   581997 non-null  float64\n",
      " 3   bin_2   582070 non-null  float64\n",
      " 4   bin_3   581986 non-null  object \n",
      " 5   bin_4   581953 non-null  object \n",
      " 6   nom_0   581748 non-null  object \n",
      " 7   nom_1   581844 non-null  object \n",
      " 8   nom_2   581965 non-null  object \n",
      " 9   nom_3   581879 non-null  object \n",
      " 10  nom_4   581965 non-null  object \n",
      " 11  nom_5   582222 non-null  object \n",
      " 12  nom_6   581869 non-null  object \n",
      " 13  nom_7   581997 non-null  object \n",
      " 14  nom_8   582245 non-null  object \n",
      " 15  nom_9   581927 non-null  object \n",
      " 16  ord_0   581712 non-null  float64\n",
      " 17  ord_1   581959 non-null  object \n",
      " 18  ord_2   581925 non-null  object \n",
      " 19  ord_3   582084 non-null  object \n",
      " 20  ord_4   582070 non-null  object \n",
      " 21  ord_5   582287 non-null  object \n",
      " 22  day     582048 non-null  float64\n",
      " 23  month   582012 non-null  float64\n",
      " 24  target  600000 non-null  int64  \n",
      "dtypes: float64(6), int64(2), object(17)\n",
      "memory usage: 114.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of null values in train data are:  0\n",
      "total number of null values in test data are:  0\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Lets fill in the Null value for train\n",
    "for i in train.columns:\n",
    "    if train[i].isnull().sum()>0:\n",
    "        train[i].fillna(train[i].mode()[0],inplace=True)\n",
    "        \n",
    "# Lets fill in the Null value for train\n",
    "for i in test.columns:\n",
    "    if test[i].isnull().sum()>0:\n",
    "        test[i].fillna(test[i].mode()[0],inplace=True)\n",
    "# Check for Null Values\n",
    "print('total number of null values in train data are: ',train.isnull().sum().sum())\n",
    "print('total number of null values in test data are: ',test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    487677\n",
       "1    112323\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chek if the train data is balanced or imbalanced\n",
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Data shape:(600000, 26) and Test Data shape:(400000, 25)\n",
      "traintest shape after combining Train and Test  (1000000, 24)\n",
      "Wall time: 417 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Preprocessing of Train and test Data together\n",
    "#Concatenate Train and Test Data with a new column to identify Train (Type=0) VS Test Data(Type=1)  \n",
    "train['Type']=pd.DataFrame(np.zeros(len(train)).astype(int)) ## 0 for train and 1 for test data\n",
    "test['Type']=pd.DataFrame(np.ones(len(test)).astype(int))\n",
    "\n",
    "print('Original Train Data shape:{} and Test Data shape:{}'.format(train.shape,test.shape))\n",
    "traintest=pd.concat([train.drop(columns=['target','id']),test.drop(columns=['id'])])\n",
    "print('traintest shape after combining Train and Test ',traintest.shape)\n",
    "label=train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    487677\n",
       "1    112323\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000000 entries, 0 to 399999\n",
      "Data columns (total 24 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   bin_0   1000000 non-null  float64\n",
      " 1   bin_1   1000000 non-null  float64\n",
      " 2   bin_2   1000000 non-null  float64\n",
      " 3   bin_3   1000000 non-null  object \n",
      " 4   bin_4   1000000 non-null  object \n",
      " 5   nom_0   1000000 non-null  object \n",
      " 6   nom_1   1000000 non-null  object \n",
      " 7   nom_2   1000000 non-null  object \n",
      " 8   nom_3   1000000 non-null  object \n",
      " 9   nom_4   1000000 non-null  object \n",
      " 10  nom_5   1000000 non-null  object \n",
      " 11  nom_6   1000000 non-null  object \n",
      " 12  nom_7   1000000 non-null  object \n",
      " 13  nom_8   1000000 non-null  object \n",
      " 14  nom_9   1000000 non-null  object \n",
      " 15  ord_0   1000000 non-null  float64\n",
      " 16  ord_1   1000000 non-null  object \n",
      " 17  ord_2   1000000 non-null  object \n",
      " 18  ord_3   1000000 non-null  object \n",
      " 19  ord_4   1000000 non-null  object \n",
      " 20  ord_5   1000000 non-null  object \n",
      " 21  day     1000000 non-null  float64\n",
      " 22  month   1000000 non-null  float64\n",
      " 23  Type    1000000 non-null  int32  \n",
      "dtypes: float64(6), int32(1), object(17)\n",
      "memory usage: 186.9+ MB\n"
     ]
    }
   ],
   "source": [
    "traintest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Lets apply oneHotEncoding\n",
    "traintest_OHE = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\n",
    "train_ohe = traintest_OHE[traintest_OHE.Type_1==0].drop(columns='Type_1')\n",
    "test_ohe = traintest_OHE[traintest_OHE.Type_1==1].drop(columns='Type_1')\n",
    "\n",
    "#Convert dataframe to spare matrix\n",
    "train_ohe = train_ohe.sparse.to_coo().tocsr()\n",
    "test_ohe = test_ohe.sparse.to_coo().tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<600000x5678 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 10230886 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try to balance the dataset by combination of over and under sampling- SMOTETomek\n",
    "imb=RandomUnderSampler(random_state=42)\n",
    "#imb=SMOTETomek(random_state=42)\n",
    "X=train_ohe\n",
    "y=label\n",
    "train_ohe,label=imb.fit_sample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224646, 5678)\n",
      "(224646,)\n"
     ]
    }
   ],
   "source": [
    "print(train_ohe.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=train_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets perform Cross validations considering all features and see what could be the best score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score,classification_report,confusion_matrix\n",
    "#from sklearn import metrics\n",
    "\n",
    "def stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y):\n",
    "    global df_model_selection\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits, random_state=12,shuffle=True)\n",
    "    \n",
    "    weighted_f1_score = []\n",
    "    #print(skf.split(X,y))\n",
    "    for train_index, test_index in skf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index] \n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        \n",
    "        model_obj.fit(X_train, y_train)\n",
    "        test_ds_predicted = model_obj.predict( X_test )      \n",
    "        weighted_f1_score.append(round(f1_score(y_true=y_test, y_pred=test_ds_predicted , average='weighted'),2))\n",
    "        \n",
    "    sd_weighted_f1_score = np.std(weighted_f1_score, ddof=1)\n",
    "    range_of_f1_scores = \"{}-{}\".format(min(weighted_f1_score),max(weighted_f1_score))    \n",
    "    df_model_selection = pd.concat([df_model_selection,pd.DataFrame([[process,model_name,sorted(weighted_f1_score),range_of_f1_scores,sd_weighted_f1_score]], columns =COLUMN_NAMES) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.77 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>F1 Scores</th>\n",
       "      <th>Range of F1 Scores</th>\n",
       "      <th>Std Deviation of F1 Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALl Features</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.7, 0.7, 0.7, 0.71, 0.71]</td>\n",
       "      <td>0.7-0.71</td>\n",
       "      <td>0.005477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Process           Model Name                    F1 Scores  \\\n",
       "0  ALl Features  Logistic Regression  [0.7, 0.7, 0.7, 0.71, 0.71]   \n",
       "\n",
       "  Range of F1 Scores  Std Deviation of F1 Scores  \n",
       "0           0.7-0.71                    0.005477  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "COLUMN_NAMES = [\"Process\",\"Model Name\", \"F1 Scores\",\"Range of F1 Scores\",\"Std Deviation of F1 Scores\"]\n",
    "df_model_selection = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "\n",
    "features=train_ohe\n",
    "\n",
    "process='ALl Features'\n",
    "n_splits = 5\n",
    "#X=SC.fit_transform(features)\n",
    "X=features\n",
    "y=label\n",
    "\n",
    "# # 1.Naive Bayes\n",
    "# model_NB=BernoulliNB()\n",
    "# model_obj=model_NB\n",
    "# model_name='Naive Bayes'\n",
    "# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "# # 2.Logistic Regression\n",
    "model_LR=LogisticRegression()\n",
    "model_obj=model_LR\n",
    "model_name='Logistic Regression'\n",
    "stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "# # # # 3.Decesion Tree Classifier\n",
    "# model_DTC=DecisionTreeClassifier()\n",
    "# model_obj=model_DTC\n",
    "# model_name='Decesion Tree Classifier'\n",
    "# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "# # 4.Random Forest Classifier\n",
    "# model_RFC=RandomForestClassifier()\n",
    "# model_obj=model_RFC\n",
    "# model_name='Random Forest Classifier'\n",
    "# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "# # 5.XGBoost Classifier\n",
    "# model_XGBC=XGBClassifier()\n",
    "# model_obj=model_XGBC\n",
    "# model_name='XGBoost Classifier'\n",
    "# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "# # 6.Gradient Boosting Classifier\n",
    "# model_GBC=GradientBoostingClassifier()\n",
    "# model_obj=model_GBC\n",
    "# model_name='Gradient Boosting Classifier'\n",
    "# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "# # 7.XGBoost Random Forest Classifier\n",
    "# model_XGBRFC=XGBRFClassifier()\n",
    "# model_obj=model_XGBRFC\n",
    "# model_name='XGBoost Random Forest Classifier'\n",
    "# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "# 8.Support Vector Machine Classifier\n",
    "# model_SVC=SVC()\n",
    "# model_obj=model_SVC\n",
    "# model_name='Support Vector Machine Classifier'\n",
    "# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "\n",
    "# 9.SGD Classifier\n",
    "# model_sgd = OneVsRestClassifier(SGDClassifier())\n",
    "# model_obj=model_sgd\n",
    "# model_name='Stochastic Gradient Descent Classifier'\n",
    "# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "#10.Gausian Process Classifier\n",
    "# model_GPC = GaussianProcessClassifier()\n",
    "# model_obj=model_GPC\n",
    "# model_name='Gausian Process Classifier'\n",
    "# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "#11.Gausian Process Classifier\n",
    "# model_KNNC=KNeighborsClassifier()\n",
    "# model_obj=model_KNNC\n",
    "# model_name='K Nearst Neighbour Classifier'\n",
    "# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "#12 Linear Discriminant Analysis\n",
    "# model_LDA=LinearDiscriminantAnalysis()\n",
    "# model_obj=model_LDA\n",
    "# model_name='Linear Discriminant Analysis'\n",
    "# stratified_K_fold_validation(model_obj, model_name, process, n_splits, X, y)\n",
    "\n",
    "#Exporting the results to csv\n",
    "#df_model_selection.to_csv(\"Model_statistics.csv\",index = False)\n",
    "df_model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1-Score: 0.73, Test f1-score: 0.71, for Sample Split: 1\n",
      "Train f1-Score: 0.73, Test f1-score: 0.7, for Sample Split: 2\n",
      "Train f1-Score: 0.73, Test f1-score: 0.7, for Sample Split: 3\n",
      "Train f1-Score: 0.73, Test f1-score: 0.7, for Sample Split: 4\n",
      "Train f1-Score: 0.73, Test f1-score: 0.71, for Sample Split: 5\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now lets try to get the best split score using StratifiedKFold Cross Validation\n",
    "\n",
    "#Initialize the algo\n",
    "model=LogisticRegression()\n",
    "\n",
    "#Initialize StratifiedKFold Method\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5, \n",
    "              random_state=1,\n",
    "              shuffle=True)\n",
    "\n",
    "#Initialize For Loop \n",
    "\n",
    "i=0\n",
    "for train,test in kfold.split(features,label):\n",
    "    i = i+1\n",
    "    X_train,X_test = features[train],features[test]\n",
    "    y_train,y_test = label[train],label[test]\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    test_ds_predicted=model.predict(X_test)\n",
    "    train_ds_predicted=model.predict(X_train)\n",
    "    \n",
    "    test_f1_score=round(f1_score(y_true=y_test, y_pred=test_ds_predicted , average='weighted'),2)\n",
    "    train_f1_score=round(f1_score(y_true=y_train, y_pred=train_ds_predicted , average='weighted'),2)\n",
    "    \n",
    "    #print(\"Train Score: {}, Test score: {}, for Sample Split: {}\".format(model.score(X_train,y_train),model.score(X_test,y_test),i))\n",
    "    print(\"Train f1-Score: {}, Test f1-score: {}, for Sample Split: {}\".format(train_f1_score,test_f1_score,i))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1-Score: 0.73, Test f1-score: 0.71\n",
      "Train Accuracy Score is:0.73 and  Test Accuracy Score:0.71\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Lets extract the Train and Test sample for split 1\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5, #n_splits should be equal to no of cv value in cross_val_score\n",
    "              random_state=1,\n",
    "              shuffle=True)\n",
    "i=0\n",
    "for train,test in kfold.split(features,label):\n",
    "    i = i+1\n",
    "    if i == 1:\n",
    "        X_train,X_test,y_train,y_test = features[train],features[test],label[train],label[test]\n",
    "\n",
    "#Final Model\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "test_ds_predicted=model.predict(X_test)\n",
    "train_ds_predicted=model.predict(X_train)\n",
    "\n",
    "test_f1_score=round(f1_score(y_true=y_test, y_pred=test_ds_predicted , average='weighted'),2)\n",
    "train_f1_score=round(f1_score(y_true=y_train, y_pred=train_ds_predicted , average='weighted'),2)\n",
    "print(\"Train f1-Score: {}, Test f1-score: {}\".format(train_f1_score,test_f1_score))\n",
    "\n",
    "\n",
    "train_score=np.round(model.score(X_train,y_train),2)\n",
    "test_score=np.round(model.score(X_test,y_test),2)\n",
    "print('Train Accuracy Score is:{} and  Test Accuracy Score:{}'.format(train_score,test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametr Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LogisticRegression(), n_iter=5,\n",
       "                   param_distributions={'max_iter': [100, 150],\n",
       "                                        'penalty': ['l1', 'l2'],\n",
       "                                        'solver': ['liblinear']})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid={\n",
    "            'penalty':['l1', 'l2'],\n",
    "            'solver':['liblinear'],\n",
    "            'max_iter':[100,150] \n",
    "           }\n",
    "model=LogisticRegression()\n",
    "RS = RandomizedSearchCV(estimator=model,param_distributions=param_grid,cv=5,n_iter=5)\n",
    "\n",
    "RS.fit(features,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7051449910110419"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RS.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=150, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=150, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression(max_iter=150, penalty='l1', solver='liblinear')\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[80289 32034]\n",
      " [30340 81983]]\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72    112323\n",
      "           1       0.72      0.73      0.72    112323\n",
      "\n",
      "    accuracy                           0.72    224646\n",
      "   macro avg       0.72      0.72      0.72    224646\n",
      "weighted avg       0.72      0.72      0.72    224646\n",
      "\n",
      "Wall time: 433 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Confusion Matrix and Classification Report\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "cm=confusion_matrix(y_true=label, y_pred=model.predict(features))\n",
    "CR=classification_report(y_true=label, y_pred=model.predict(features))\n",
    "print('Confusion Matrix:\\n',cm)\n",
    "print('\\n Classification Report:\\n',CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#model.predict()\n",
    "submission=model.predict(test_ohe)\n",
    "\n",
    "submission=pd.DataFrame(submission,columns=['target'])\n",
    "submission.insert(0,'id',test['id'])\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
